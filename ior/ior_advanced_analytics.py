#!/usr/bin/env python3
"""
IoR Advanced Analytics - Statistical Validation & Pattern Recognition
================================================================
Impression of Reality (IoR) - í˜„ì‹¤ì˜ ì¸ìƒì„ ë‹¤ê°ë„ë¡œ ë¶„ì„í•˜ëŠ” í†µí•© ì‹œìŠ¤í…œ

ê³ ê¸‰ í†µê³„ ë¶„ì„ì„ í†µí•œ IoR ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ë° ìµœì í™”
- ê³„ì¸µì  ì¸ê³¼ê´€ê³„ ëª¨ë¸ë§ (Voodoo/Psychologyâ†’Sajuâ†’Climateâ†’Astrologyâ†’I Ching)
- ë² ì´ì§€ì•ˆ ì¶”ë¡  ë° ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ íŒ¨í„´ ì¸ì‹
- êµì°¨ ê²€ì¦ ë° ëª¨ë¸ ìµœì í™”
- ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì •í™•ë„ ê°œì„ 

"""

import json
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import pearsonr, spearmanr
import warnings
warnings.filterwarnings('ignore')

class AstrologyComputationalReinforcement:
    """ì ì„±ìˆ  ì‹œìŠ¤í…œì˜ ìˆ˜í•™ì /ê³„ì‚°ì  ë³´ê°• í´ë˜ìŠ¤
    
    ëª©ì : ê¸°ë¡ì´ ë¶€ì¡±í•˜ê³  ê°œë°œì´ ëœ ëœ ì ì„±ìˆ ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë³´ê°•í•˜ì—¬
         IoR ì‹œìŠ¤í…œì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” "ë²½ëŒ"ë¡œ ë§Œë“¤ê¸°
    """
    
    def __init__(self):
        # í–‰ì„±ë³„ ì˜í–¥ë ¥ ê°€ì¤‘ì¹˜ (ê²½í—˜ì  ë°ì´í„° ê¸°ë°˜)
        self.planetary_weights = {
            'sun': 0.25,      # ê°œì¸ì„±, ìì•„
            'moon': 0.20,     # ê°ì •, ì§ê´€
            'venus': 0.18,    # ì‚¬ë‘, ê´€ê³„
            'mars': 0.12,     # í–‰ë™, ì—´ì •
            'jupiter': 0.10,  # í™•ì¥, í–‰ìš´
            'saturn': 0.08,   # ì œí•œ, ì±…ì„
            'mercury': 0.07   # ì†Œí†µ, ì‚¬ê³ 
        }
        
        # í•˜ìš°ìŠ¤ë³„ ê´€ê³„ ì˜í–¥ë„
        self.house_relationship_impact = {
            1: 0.15,  # ìì•„, ì²«ì¸ìƒ
            5: 0.20,  # ë¡œë§¨ìŠ¤, ì°½ì¡°ì„±
            7: 0.25,  # íŒŒíŠ¸ë„ˆì‹­, ê²°í˜¼ (ê°€ì¥ ì¤‘ìš”)
            8: 0.15,  # ë³€í™”, ê¹Šì€ ê²°í•©
            9: 0.10,  # ì² í•™, ê°€ì¹˜ê´€
            10: 0.08, # ëª…ì„±, ê³µì  ê´€ê³„
            11: 0.07  # ìš°ì •, ì‚¬íšŒì  ê´€ê³„
        }
        
        # ìƒìœ„ë³„ í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤
        self.aspect_compatibility = {
            'conjunction': 0.8,   # 0ë„ - ê°•í•œ ê²°í•©
            'trine': 0.9,        # 120ë„ - ì¡°í™”
            'sextile': 0.7,      # 60ë„ - ê¸°íšŒ
            'square': 0.3,       # 90ë„ - ë„ì „
            'opposition': 0.4,   # 180ë„ - ê¸´ì¥
            'quincunx': 0.2      # 150ë„ - ì¡°ì • í•„ìš”
        }
    
    def calculate_planetary_positions(self, birth_date: str, birth_time: str = "12:00", 
                                    birth_location: str = "Seoul") -> Dict[str, float]:
        """í–‰ì„± ìœ„ì¹˜ ê³„ì‚° (ê°„ì†Œí™”ëœ ì—í˜ë©”ë¦¬ìŠ¤ ê·¼ì‚¬)"""
        # ì‹¤ì œë¡œëŠ” SwissEphë‚˜ pyephem ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ, 
        # ì—¬ê¸°ì„œëŠ” ë‚ ì§œ ê¸°ë°˜ ê·¼ì‚¬ ê³„ì‚°
        
        from datetime import datetime
        date_obj = datetime.strptime(birth_date, '%Y-%m-%d')
        day_of_year = date_obj.timetuple().tm_yday
        
        # ê°„ì†Œí™”ëœ í–‰ì„± ìœ„ì¹˜ ê³„ì‚° (ì‹¤ì œ ì²œë¬¸í•™ì  ê³„ì‚°ì˜ ê·¼ì‚¬)
        positions = {}
        for planet, base_speed in [
            ('sun', 1.0),      # 1ë„/ì¼
            ('moon', 13.2),    # 13.2ë„/ì¼
            ('mercury', 1.6),  # í‰ê·  1.6ë„/ì¼
            ('venus', 1.2),    # í‰ê·  1.2ë„/ì¼
            ('mars', 0.5),     # í‰ê·  0.5ë„/ì¼
            ('jupiter', 0.08), # í‰ê·  0.08ë„/ì¼
            ('saturn', 0.03)   # í‰ê·  0.03ë„/ì¼
        ]:
            # 2024ë…„ ê¸°ì¤€ì ì—ì„œì˜ ìœ„ì¹˜ ê³„ì‚°
            reference_position = hash(planet) % 360  # ê¸°ì¤€ ìœ„ì¹˜
            position = (reference_position + day_of_year * base_speed) % 360
            positions[planet] = position
        
        return positions
    
    def calculate_aspects(self, positions1: Dict[str, float], 
                         positions2: Dict[str, float]) -> Dict[str, float]:
        """ë‘ ì¶œìƒì°¨íŠ¸ ê°„ì˜ ìƒìœ„(aspects) ê³„ì‚°"""
        aspects_score = []
        
        for planet1, pos1 in positions1.items():
            for planet2, pos2 in positions2.items():
                angle_diff = abs(pos1 - pos2)
                if angle_diff > 180:
                    angle_diff = 360 - angle_diff
                
                # ìƒìœ„ íŒì • (ì˜¤ë¸Œ 10ë„ í—ˆìš©)
                for aspect_name, aspect_angle in [
                    ('conjunction', 0), ('sextile', 60), ('square', 90),
                    ('trine', 120), ('quincunx', 150), ('opposition', 180)
                ]:
                    if abs(angle_diff - aspect_angle) <= 10:
                        # í–‰ì„±ë³„ ê°€ì¤‘ì¹˜ ì ìš©
                        weight1 = self.planetary_weights.get(planet1, 0.05)
                        weight2 = self.planetary_weights.get(planet2, 0.05)
                        compatibility = self.aspect_compatibility[aspect_name]
                        
                        aspect_strength = weight1 * weight2 * compatibility
                        aspects_score.append(aspect_strength)
                        break
        
        return {
            'total_aspects': len(aspects_score),
            'average_compatibility': np.mean(aspects_score) if aspects_score else 0.0,
            'weighted_score': sum(aspects_score)
        }
    
    def calculate_composite_chart_strength(self, birth_date1: str, birth_date2: str) -> float:
        """í•©ì„± ì°¨íŠ¸ ê°•ë„ ê³„ì‚°"""
        pos1 = self.calculate_planetary_positions(birth_date1)
        pos2 = self.calculate_planetary_positions(birth_date2)
        
        # í•©ì„± ì°¨íŠ¸ ì¤‘ì  ê³„ì‚°
        composite_positions = {}
        for planet in pos1.keys():
            midpoint = (pos1[planet] + pos2[planet]) / 2
            # 180ë„ë¥¼ ë„˜ëŠ” ê²½ìš° ì²˜ë¦¬
            if abs(pos1[planet] - pos2[planet]) > 180:
                midpoint = (midpoint + 180) % 360
            composite_positions[planet] = midpoint
        
        # í•©ì„± ì°¨íŠ¸ ë‚´ë¶€ ì¡°í™”ë„ ê³„ì‚°
        internal_harmony = 0
        aspect_count = 0
        
        planets = list(composite_positions.keys())
        for i, planet1 in enumerate(planets):
            for planet2 in planets[i+1:]:
                angle_diff = abs(composite_positions[planet1] - composite_positions[planet2])
                if angle_diff > 180:
                    angle_diff = 360 - angle_diff
                
                # ì¡°í™”ë¡œìš´ ê°ë„ì¸ì§€ í™•ì¸
                for aspect_name, aspect_angle in [('trine', 120), ('sextile', 60)]:
                    if abs(angle_diff - aspect_angle) <= 8:
                        weight1 = self.planetary_weights.get(planet1, 0.05)
                        weight2 = self.planetary_weights.get(planet2, 0.05)
                        internal_harmony += weight1 * weight2 * self.aspect_compatibility[aspect_name]
                        aspect_count += 1
        
        return internal_harmony / max(1, aspect_count)
    
    def reinforced_astrology_score(self, person1_data: Dict, person2_data: Dict) -> float:
        """ë³´ê°•ëœ ì ì„±ìˆ  ì ìˆ˜ ê³„ì‚°"""
        birth_date1 = person1_data.get('birth_date', '1990-01-01')
        birth_date2 = person2_data.get('birth_date', '1990-01-01')
        
        # 1. ê¸°ë³¸ ìƒìœ„ í˜¸í™˜ì„±
        pos1 = self.calculate_planetary_positions(birth_date1)
        pos2 = self.calculate_planetary_positions(birth_date2)
        aspects_result = self.calculate_aspects(pos1, pos2)
        
        # 2. í•©ì„± ì°¨íŠ¸ ê°•ë„
        composite_strength = self.calculate_composite_chart_strength(birth_date1, birth_date2)
        
        # 3. ê´€ê³„ í•˜ìš°ìŠ¤ íŠ¹ë³„ ê°€ì¤‘ì¹˜ (7í•˜ìš°ìŠ¤, 5í•˜ìš°ìŠ¤ ë“±)
        relationship_factor = 0
        for planet, position in pos1.items():
            house = int(position / 30) + 1  # ê°„ë‹¨í•œ í•˜ìš°ìŠ¤ ê³„ì‚°
            if house in self.house_relationship_impact:
                relationship_factor += (self.planetary_weights.get(planet, 0.05) * 
                                      self.house_relationship_impact[house])
        
        # 4. ìµœì¢… ë³´ê°•ëœ ì ìˆ˜
        base_score = aspects_result['weighted_score']
        composite_bonus = composite_strength * 0.3
        relationship_bonus = relationship_factor * 0.2
        
        reinforced_score = (base_score + composite_bonus + relationship_bonus) * 1.2
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        return min(1.0, max(0.0, reinforced_score))

class VoodooPsychologyComputationalReinforcement:
    """ë¶€ë‘/ì‹¬ë¦¬í•™ ì‹œìŠ¤í…œì˜ ìˆ˜í•™ì /ê³„ì‚°ì  ë³´ê°• í´ë˜ìŠ¤
    
    ëª©ì : ì¶”ìƒì ì¸ ë¶€ë‘/ì‹¬ë¦¬í•™ì„ ì •ëŸ‰ì  ì‹¬ë¦¬í•™ ëª¨ë¸ë¡œ ë³´ê°•í•˜ì—¬
         ë©”íƒ€â†’í˜„ì‹¤ ë³€í™˜ì˜ ê³„ì‚° ê°€ëŠ¥í•œ "ë²½ëŒ"ë¡œ ë§Œë“¤ê¸°
    """
    
    def __init__(self):
        # ì‹¬ë¦¬í•™ì  í˜¸í™˜ì„± ì°¨ì›ë“¤ (Big Five + ì¶”ê°€)
        self.psychological_dimensions = {
            'openness': {'weight': 0.18, 'optimal_diff': 0.2},          # ê°œë°©ì„±
            'conscientiousness': {'weight': 0.22, 'optimal_diff': 0.1}, # ì„±ì‹¤ì„±  
            'extraversion': {'weight': 0.15, 'optimal_diff': 0.3},      # ì™¸í–¥ì„±
            'agreeableness': {'weight': 0.20, 'optimal_diff': 0.1},     # ì¹œí™”ì„±
            'neuroticism': {'weight': 0.10, 'optimal_diff': 0.0},       # ì‹ ê²½ì„± (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            'attachment_style': {'weight': 0.15, 'optimal_diff': 0.0}    # ì• ì°© ìœ í˜•
        }
        
        # ê´€ê³„ ì„±ê³µ ì˜ˆì¸¡ ì¸ìë“¤
        self.relationship_predictors = {
            'communication_compatibility': 0.25,
            'conflict_resolution_style': 0.20,
            'value_alignment': 0.20,
            'emotional_intelligence_match': 0.15,
            'lifestyle_compatibility': 0.12,
            'future_goal_alignment': 0.08
        }
        
        # ë©”íƒ€í˜„ì‹¤ ë³€í™˜ ê³„ìˆ˜ (ë¶€ë‘ì  ê°œë…ì˜ ìˆ˜í•™ì  ëª¨ë¸ë§)
        self.meta_reality_coefficients = {
            'intention_strength': 0.3,      # ì˜ë„ì˜ ê°•ë„
            'belief_consistency': 0.25,     # ë¯¿ìŒì˜ ì¼ê´€ì„±
            'emotional_investment': 0.2,    # ê°ì •ì  íˆ¬ì
            'symbolic_resonance': 0.15,     # ìƒì§•ì  ê³µëª…
        }
    
    def calculate_correlation_matrix(self, df: pd.DataFrame) -> pd.DataFrame:
        """íŠ¹ì„± ê°„ ìƒê´€ê´€ê³„ í–‰ë ¬ ê³„ì‚°"""
        correlation_matrix = df.corr(method='pearson')
        return correlation_matrix
    
    def correlation_analysis(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„"""
        print("\nğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„")
        print("=" * 30)
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        correlation_matrix = self.df[numeric_cols].corr()
        
        # ì‹œê°í™”
        plt.figure(figsize=(10, 8))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
        plt.title('IoR ì‹œìŠ¤í…œ ê°„ ìƒê´€ê´€ê³„')
        plt.tight_layout()
        plt.savefig('ior_correlation_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # í‰ê·  ìƒê´€ê´€ê³„
        system_correlations = correlation_matrix.loc[self.system_names, self.system_names]
        avg_correlation = np.mean(np.abs(system_correlations.values[np.triu_indices_from(system_correlations.values, k=1)]))
        
        return {
            'correlation_matrix': correlation_matrix.to_dict(),
            'average_correlation': avg_correlation,
            'strongest_correlation': system_correlations.abs().max().max(),
            'system_correlations': system_correlations.to_dict()
        }
    
    def predictive_modeling(self) -> Dict[str, Any]:
        """ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸ë§"""
        print("\nğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸ë§")
        print("=" * 40)
        
        # íŠ¹ì„± ë° íƒ€ê²Ÿ ë¶„ë¦¬
        X = self.df[self.system_names]
        y = self.df['target']
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸
        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
        rf_model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
        y_pred = rf_model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        # íŠ¹ì„± ì¤‘ìš”ë„
        feature_importance = dict(zip(self.system_names, rf_model.feature_importances_))
        
        print(f"ëª¨ë¸ ì„±ëŠ¥: RÂ²={r2:.4f}, MSE={mse:.4f}")
        
        return {
            'model_performance': {'r2_score': r2, 'mse': mse},
            'feature_importance': feature_importance
        }
    
    def bayesian_inference(self) -> Dict[str, Any]:
        """ë² ì´ì§€ì•ˆ ì¶”ë¡  ë¶„ì„"""
        print("\nğŸ”® ë² ì´ì§€ì•ˆ ì¶”ë¡  ë¶„ì„")
        print("=" * 30)
        
        bayesian_results = {}
        
        for system in self.system_names:
            scores = self.df[system].values
            
            # ë² íƒ€ ë¶„í¬ íŒŒë¼ë¯¸í„° ì¶”ì •
            mean_score = np.mean(scores)
            var_score = np.var(scores)
            
            if var_score > 0 and mean_score > 0 and mean_score < 1:
                alpha = mean_score * ((mean_score * (1 - mean_score)) / var_score - 1)
                beta = (1 - mean_score) * ((mean_score * (1 - mean_score)) / var_score - 1)
                
                confidence_interval = stats.beta.interval(0.95, alpha, beta)
                prob_above_half = 1 - stats.beta.cdf(0.5, alpha, beta)
                
                bayesian_results[system] = {
                    'alpha': alpha,
                    'beta': beta,
                    'mean': mean_score,
                    'variance': var_score,
                    'confidence_interval_95': confidence_interval,
                    'probability_above_0.5': prob_above_half,
                    'reliability_score': prob_above_half * (1 - var_score)
                }
        
        return bayesian_results
    
    def optimize_weights(self) -> Dict[str, Any]:
        """ê°€ì¤‘ì¹˜ ìµœì í™”"""
        print("\nâš–ï¸ ê°€ì¤‘ì¹˜ ìµœì í™”")
        print("=" * 25)
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        optimization_results = {}
        
        for system in self.system_names:
            system_scores = self.df[system].values
            target_scores = self.df['target'].values
            
            # ìƒê´€ê´€ê³„ ê¸°ë°˜ ê°€ì¤‘ì¹˜
            correlation = np.corrcoef(system_scores, target_scores)[0, 1]
            
            # ê¸°ì¡´ ê°€ì¤‘ì¹˜ (ê· ë“±)
            original_weight = 1.0 / len(self.system_names)
            
            # ìµœì í™”ëœ ê°€ì¤‘ì¹˜ (ìƒê´€ê´€ê³„ ê¸°ë°˜)
            optimized_weight = max(0.1, abs(correlation)) / sum([abs(np.corrcoef(self.df[s].values, target_scores)[0, 1]) for s in self.system_names])
            
            # ë³€í™”ìœ¨ ê³„ì‚°
            change_percent = ((optimized_weight - original_weight) / original_weight) * 100
            
            optimization_results[system] = {
                'original_weight': original_weight,
                'optimized_weight': optimized_weight,
                'change_percent': change_percent,
                'correlation_with_target': correlation
            }
        
        return optimization_results
    
    def statistical_significance_test(self) -> Dict[str, Any]:
        """í†µê³„ì  ìœ ì˜ì„± ê²€ì •"""
        print("\nğŸ“ˆ í†µê³„ì  ìœ ì˜ì„± ê²€ì •")
        print("=" * 35)
        
        results = {}
        
        for system in self.system_names:
            scores = self.df[system].values
            
            # ë‹¨ì¼ í‘œë³¸ t-ê²€ì • (í‰ê· ì´ 0.5ì™€ ë‹¤ë¥¸ì§€)
            t_stat, p_value = stats.ttest_1samp(scores, 0.5)
            
            results[system] = {
                'mean_score': np.mean(scores),
                't_statistic': t_stat,
                'p_value': p_value,
                'significant': p_value < 0.05
            }
        
        # ANOVA (ì‹œìŠ¤í…œ ê°„ ì°¨ì´)
        system_scores = [self.df[system].values for system in self.system_names]
        f_stat, p_value_anova = stats.f_oneway(*system_scores)
        
        results['anova'] = {
            'f_statistic': f_stat,
            'p_value': p_value_anova,
            'significant_difference': p_value_anova < 0.05
        }
        
        return results
    
    def generate_comprehensive_report(self):
        """ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "="*60)
        print("ğŸ”¬ IoR ì‹œìŠ¤í…œ ê³ ê¸‰ í†µê³„ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸")
        print("="*60)
        
        # ëª¨ë“  ë¶„ì„ ì‹¤í–‰
        correlation_results = self.correlation_analysis()
        ml_results = self.predictive_modeling()
        bayesian_results = self.bayesian_inference()
        optimization_results = self.optimize_weights()
        significance_results = self.statistical_significance_test()
        
        # IoR ê³ ìœ  ë¶„ì„ ì¶”ê°€
        hierarchical_results = self.hierarchical_causality_analysis()
        causal_pathway_results = self.causal_pathway_validation()
        
        # ìˆ˜í•™ì  ì‹œìŠ¤í…œë³„ ì •ë°€ ë¶„ì„
        riemann_astrology_results = self.riemann_astrology_analysis()
        bayesian_voodoo_results = self.bayesian_voodoo_analysis()
        mathematical_alignment_results = self.mathematical_system_alignment()
        
        # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        comprehensive_report = {
            'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
            'dataset_summary': {
                'total_records': len(self.df),
                'celebrities': self.df['celebrity'].nunique(),
                'systems_analyzed': len(self.system_names)
            },
            'correlation_analysis': correlation_results,
            'machine_learning': {
                'model_performance': ml_results['model_performance'],
                'feature_importance': ml_results['feature_importance']
            },
            'bayesian_inference': bayesian_results,
            'weight_optimization': optimization_results,
            'statistical_significance': significance_results,
            'hierarchical_causality': hierarchical_results,
            'causal_pathway_validation': causal_pathway_results,
            'riemann_astrology_analysis': riemann_astrology_results,
            'bayesian_voodoo_analysis': bayesian_voodoo_results,
            'mathematical_system_alignment': mathematical_alignment_results,
            'recommendations': self._generate_recommendations(
                correlation_results, ml_results, bayesian_results, 
                optimization_results, significance_results, hierarchical_results, 
                causal_pathway_results, riemann_astrology_results, bayesian_voodoo_results
            )
        }
        
        # JSON ì €ì¥
        with open('ior_advanced_analytics_report.json', 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nâœ… ì¢…í•© ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“Š ë¦¬í¬íŠ¸ ì €ì¥: ior_advanced_analytics_report.json")
        print(f"ğŸ“ˆ ì‹œê°í™” ì €ì¥: ior_correlation_heatmap.png")
        
        return comprehensive_report
    
    def _generate_recommendations(self, corr_results, ml_results, bayesian_results, 
                                opt_results, sig_results, hierarchical_results=None, 
                                causal_results=None, riemann_results=None, voodoo_results=None) -> List[str]:
        """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # 1. ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¶”ì²œ
        if corr_results.get('average_correlation', 0) < 0.3:
            recommendations.append("ì‹œìŠ¤í…œ ê°„ ìƒê´€ê´€ê³„ê°€ ë‚®ìŒ - ë…ë¦½ì  ì˜ˆì¸¡ì´ ê°€ëŠ¥í•˜ì—¬ ì•™ìƒë¸” íš¨ê³¼ ê¸°ëŒ€")
        
        # 2. ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì¶”ì²œ
        if ml_results.get('feature_importance'):
            top_system = max(ml_results['feature_importance'].items(), key=lambda x: x[1])
            recommendations.append(f"ê°€ì¥ ì¤‘ìš”í•œ ì‹œìŠ¤í…œ: {top_system[0]} - ì´ ì‹œìŠ¤í…œì˜ ì •í™•ë„ ê°œì„ ì— ì§‘ì¤‘")
        
        # 3. ë² ì´ì§€ì•ˆ ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ
        reliable_systems = [(sys, data.get('reliability_score', 0)) 
                          for sys, data in bayesian_results.items() 
                          if isinstance(data, dict) and 'reliability_score' in data]
        if reliable_systems:
            most_reliable = max(reliable_systems, key=lambda x: x[1])
            recommendations.append(f"ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œ: {most_reliable[0]}")
        
        # 4. ê°€ì¤‘ì¹˜ ìµœì í™” ê¸°ë°˜ ì¶”ì²œ
        if opt_results:
            weight_changes = [(sys, data.get('change_percent', 0)) for sys, data in opt_results.items()]
            big_changes = [sys for sys, change in weight_changes if abs(change) > 20]
            if big_changes:
                recommendations.append(f"ê°€ì¤‘ì¹˜ ëŒ€í­ ì¡°ì • í•„ìš” ì‹œìŠ¤í…œ: {', '.join(big_changes)}")
        
        # 5. í†µê³„ì  ìœ ì˜ì„± ê¸°ë°˜ ì¶”ì²œ
        non_significant = [sys for sys, data in sig_results.items() 
                          if sys != 'anova' and not data.get('significant', False)]
        if non_significant:
            recommendations.append(f"í†µê³„ì  ìœ ì˜ì„± ë¶€ì¡± ì‹œìŠ¤í…œ: {', '.join(non_significant)} - ì•Œê³ ë¦¬ì¦˜ ê°œì„  í•„ìš”")
        
        # 6. ê³„ì¸µì  ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ
        if hierarchical_results:
            layer_analysis = hierarchical_results.get('layer_analysis', {})
            if layer_analysis:
                layer_items = list(layer_analysis.items())
                if layer_items:
                    strongest_layer = max(layer_items, key=lambda x: x[1].get('layer_strength', 0))
                    recommendations.append(f"ê°€ì¥ ê°•í•œ ê³„ì¸µ: {strongest_layer[0]} - ì´ ê³„ì¸µ ì‹œìŠ¤í…œë“¤ì˜ ì •í™•ë„ ìš°ì„  ê°œì„ ")
            
            hierarchical_effectiveness = hierarchical_results.get('hierarchical_effectiveness', 0)
            if hierarchical_effectiveness < 0.5:
                recommendations.append("ê³„ì¸µì  êµ¬ì¡°ì˜ íš¨ê³¼ì„±ì´ ë‚®ìŒ - ê³„ì¸µ ê°„ ì—°ê²° ê°•í™” í•„ìš”")
        
        # 7. ë¦¬ë§Œ ì ì„±ìˆ  ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ
        if riemann_results:
            avg_curvature = riemann_results.get('average_curvature_strength', 0)
            if avg_curvature > 0.7:
                recommendations.append("ë†’ì€ ìš°ì£¼ì  ê³¡ë¥  ê°ì§€ - ì ì„±ìˆ  ì‹œìŠ¤í…œì˜ ê¸°í•˜í•™ì  ì •ë°€ë„ í™œìš© ê°•í™”")
            elif avg_curvature < 0.3:
                recommendations.append("ë‚®ì€ ìš°ì£¼ì  ê³¡ë¥  - ì ì„±ìˆ ë³´ë‹¤ ë‹¤ë¥¸ ì‹œìŠ¤í…œë“¤ì— ë” ì˜ì¡´í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ")
        
        # 8. ë² ì´ì§€ì•ˆ ë¶€ë‘ ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ
        if voodoo_results:
            for voodoo_system, data in voodoo_results.items():
                if isinstance(data, dict):
                    reality_strength = data.get('reality_manipulation_strength', 0)
                    intervention_effect = data.get('direct_intervention_effect', 0)
                    
                    if reality_strength > 0.8:
                        recommendations.append(f"{voodoo_system}: ê°•ë ¥í•œ í˜„ì‹¤ ì¡°ì‘ ëŠ¥ë ¥ - ì§ì ‘ ê°œì… ì „ëµ í™œìš©")
                    elif intervention_effect > 0.3:
                        recommendations.append(f"{voodoo_system}: ìœ ì˜ë¯¸í•œ ê°œì… íš¨ê³¼ - ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸ ë¡œì§ ê°•í™”")
                    
                    if data.get('statistical_significance', False):
                        max_target = data.get('max_manipulation_target')
                        if max_target:
                            target_name, target_weight = max_target
                            recommendations.append(f"ìµœì  ê°œì… ëŒ€ìƒ: {target_name} (ê°€ì¤‘ì¹˜: {target_weight:.3f}) - ìš°ì„  ì ìš© ëŒ€ìƒ")
                    
                    intervention_classification = data.get('intervention_classification', '')
                    if 'í˜„ì‹¤ ì¡°ì‘ ìˆ˜ì¤€' in intervention_classification:
                        recommendations.append("âš ï¸ í˜„ì‹¤ ì¡°ì‘ ìˆ˜ì¤€ì˜ ê°œì… ëŠ¥ë ¥ - ìœ¤ë¦¬ì  ì‚¬ìš© ê³ ë ¤ í•„ìš”")
        
        return recommendations
        big_changes = [sys for sys, change in weight_changes if abs(change) > 20]
        if big_changes:
            recommendations.append(f"ê°€ì¤‘ì¹˜ ëŒ€í­ ì¡°ì • í•„ìš” ì‹œìŠ¤í…œ: {', '.join(big_changes)}")
        
        # 5. í†µê³„ì  ìœ ì˜ì„± ê¸°ë°˜ ì¶”ì²œ
        non_significant = [sys for sys, data in sig_results.items() 
                          if sys != 'anova' and not data.get('significant', False)]
        if non_significant:
            recommendations.append(f"í†µê³„ì  ìœ ì˜ì„± ë¶€ì¡± ì‹œìŠ¤í…œ: {', '.join(non_significant)} - ì•Œê³ ë¦¬ì¦˜ ê°œì„  í•„ìš”")
        
        # 6. ê³„ì¸µì  ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ
        if hierarchical_results:
            layer_analysis = hierarchical_results.get('layer_analysis', {})
            if layer_analysis:
                strongest_layer = max(layer_analysis.items(), key=lambda x: x[1]['layer_strength'])
                recommendations.append(f"ê°€ì¥ ê°•í•œ ê³„ì¸µ: {strongest_layer[0]} - ì´ ê³„ì¸µ ì‹œìŠ¤í…œë“¤ì˜ ì •í™•ë„ ìš°ì„  ê°œì„ ")
            
            hierarchical_effectiveness = hierarchical_results.get('hierarchical_effectiveness', 0)
            if hierarchical_effectiveness < 0.5:
                recommendations.append("ê³„ì¸µì  êµ¬ì¡°ì˜ íš¨ê³¼ì„±ì´ ë‚®ìŒ - ê³„ì¸µ ê°„ ì—°ê²° ê°•í™” í•„ìš”")
        
        # 7. ì¸ê³¼ê´€ê³„ ê²½ë¡œ ê¸°ë°˜ ì¶”ì²œ
        if causal_results:
            pathway_validity = causal_results.get('pathway_validity_rate', 0)
            if pathway_validity < 0.5:
                recommendations.append("ì¸ê³¼ê´€ê³„ ê²½ë¡œ ìœ íš¨ì„± ë¶€ì¡± - ê³„ì¸µ ìˆœì„œ ì¬ê²€í†  í•„ìš”")
            
            strongest_pathway = causal_results.get('strongest_pathway')
            if strongest_pathway:
                pathway_name = strongest_pathway[0].replace('_to_', ' â†’ ').replace('_', ' ').title()
                recommendations.append(f"ê°€ì¥ ê°•í•œ ì¸ê³¼ê´€ê³„: {pathway_name} - ì´ ê²½ë¡œ ìµœì í™” ìš°ì„ ")
        
        return recommendations

    def hierarchical_causality_analysis(self) -> Dict[str, Any]:
        """ê³„ì¸µì  ì¸ê³¼ê´€ê³„ ë¶„ì„"""
        print("\nğŸ”— ê³„ì¸µì  ì¸ê³¼ê´€ê³„ ë¶„ì„")
        print("=" * 35)
        
        # IoR ê³„ì¸µ êµ¬ì¡° ì •ì˜
        hierarchical_layers = {
            'meta_layer': ['voodoo_psychology'],  # ë©”íƒ€ í˜„ì‹¤ ì¡°ì‘
            'foundational_layer': ['saju_four_pillars'],  # ê¸°ë³¸ ê´€ê³„ íŒ¨í„´
            'environmental_layer': ['climate_science'],  # í™˜ê²½ ì˜í–¥
            'cosmic_layer': ['western_astrology', 'vedic_astrology'],  # ìš°ì£¼ì  ì˜í–¥
            'probabilistic_layer': ['i_ching', 'runic_divination']  # í™•ë¥ ì  ì—°ê²°
        }
        
        layer_analysis = {}
        
        for layer_name, systems in hierarchical_layers.items():
            available_systems = [s for s in systems if s in self.df.columns]
            
            if available_systems:
                layer_scores = []
                for system in available_systems:
                    layer_scores.extend(self.df[system].values)
                
                layer_analysis[layer_name] = {
                    'systems': available_systems,
                    'layer_mean': np.mean(layer_scores),
                    'layer_std': np.std(layer_scores),
                    'layer_strength': np.mean(layer_scores) * (1 - np.std(layer_scores))
                }
        
        # ê³„ì¸µ ê°„ ìƒí˜¸ì‘ìš© ë¶„ì„
        layer_interactions = {}
        available_layers = list(layer_analysis.keys())
        
        for i, layer1 in enumerate(available_layers):
            for layer2 in available_layers[i+1:]:
                systems1 = layer_analysis[layer1]['systems']
                systems2 = layer_analysis[layer2]['systems']
                
                if systems1 and systems2:
                    correlations = []
                    for s1 in systems1:
                        for s2 in systems2:
                            if s1 in self.df.columns and s2 in self.df.columns:
                                corr = np.corrcoef(self.df[s1], self.df[s2])[0,1]
                                if not np.isnan(corr):
                                    correlations.append(corr)
                    
                    if correlations:
                        layer_interactions[f"{layer1}_{layer2}"] = {
                            'average_correlation': np.mean(correlations),
                            'interaction_strength': abs(np.mean(correlations))
                        }
        
        # ì „ì²´ ê³„ì¸µì  íš¨ê³¼ì„±
        hierarchical_effectiveness = np.mean([data['layer_strength'] for data in layer_analysis.values()])
        
        return {
            'layer_analysis': layer_analysis,
            'layer_interactions': layer_interactions,
            'hierarchical_effectiveness': hierarchical_effectiveness
        }
    
    def causal_pathway_validation(self) -> Dict[str, Any]:
        """ì¸ê³¼ê´€ê³„ ê²½ë¡œ ê²€ì¦"""
        print("\nğŸ›¤ï¸ ì¸ê³¼ê´€ê³„ ê²½ë¡œ ê²€ì¦")
        print("=" * 35)
        
        # ì¸ê³¼ê´€ê³„ ê°€ì„¤ ê²½ë¡œ
        causal_pathways = [
            {
                'name': 'meta_to_foundation',
                'path': ['voodoo_psychology', 'saju_four_pillars'],
                'hypothesis': 'ë©”íƒ€ í˜„ì‹¤ ì¡°ì‘ì´ ê¸°ë³¸ ê´€ê³„ íŒ¨í„´ì— ì˜í–¥'
            },
            {
                'name': 'cosmic_to_probabilistic', 
                'path': ['western_astrology', 'i_ching'],
                'hypothesis': 'ìš°ì£¼ì  íŒ¨í„´ì´ í™•ë¥ ì  ì—°ê²°ì— ì˜í–¥'
            },
            {
                'name': 'foundation_to_outcome',
                'path': ['saju_four_pillars', 'target'],
                'hypothesis': 'ê¸°ë³¸ ê´€ê³„ íŒ¨í„´ì´ ìµœì¢… ê²°ê³¼ì— ì˜í–¥'
            }
        ]
        
        pathway_results = {}
        
        for pathway in causal_pathways:
            path_systems = pathway['path']
            available_path = [s for s in path_systems if s in self.df.columns]
            
            if len(available_path) >= 2:
                # ê²½ë¡œ ê°•ë„ ê³„ì‚° (ìˆœì°¨ì  ìƒê´€ê´€ê³„)
                path_correlations = []
                for i in range(len(available_path) - 1):
                    sys1, sys2 = available_path[i], available_path[i+1]
                    corr = np.corrcoef(self.df[sys1], self.df[sys2])[0,1]
                    path_correlations.append(corr)
                
                # ê²½ë¡œì˜ ì „ì²´ ê°•ë„ (ìƒê´€ê´€ê³„ë“¤ì˜ ê¸°í•˜í‰ê· )
                path_strength = np.prod([abs(c) for c in path_correlations]) ** (1/len(path_correlations))
                
                pathway_results[pathway['name']] = {
                    'hypothesis': pathway['hypothesis'],
                    'available_path': available_path,
                    'path_correlations': path_correlations,
                    'path_strength': path_strength,
                    'validated': path_strength > 0.3  # ì„ê³„ê°’
                }
        
        return pathway_results
    def riemann_astrology_analysis(self) -> Dict[str, Any]:
        """ì ì„±ìˆ ì˜ ë¦¬ë§Œ ë°©ì •ì‹ì  íŠ¹ì„± ë¶„ì„ - ì‹œê³µê°„ ê³¡ë¥ ê³¼ í–‰ì„± ë°°ì¹˜ì˜ ê¸°í•˜í•™ì  ê´€ê³„"""
        print("\nğŸŒŒ ì ì„±ìˆ  ë¦¬ë§Œ ê¸°í•˜í•™ì  ë¶„ì„")
        print("=" * 45)
        print("ì‹œê³µê°„ ê³¡ë¥  â†’ í–‰ì„± ë°°ì¹˜ â†’ í˜„ì‹¤ êµ¬ì¡°í™”")
        
        astrology_scores = self.df['western_astrology'].values
        vedic_scores = self.df['vedic_astrology'].values
        
        # ë¦¬ë§Œ ê³¡ë¥  í…ì„œ ëª¨ë°© - ì ì„±ìˆ  ì‹œìŠ¤í…œ ê°„ì˜ ê³¡ë¥  ê³„ì‚°
        curvature_matrix = np.zeros((len(astrology_scores), len(astrology_scores)))
        
        for i in range(len(astrology_scores)):
            for j in range(len(astrology_scores)):
                if i != j:
                    # ê³¡ë¥  ê³„ì‚°: ë‘ ì  ì‚¬ì´ì˜ ê¸°í•˜í•™ì  "íœ˜ì–´ì§"
                    spatial_distance = abs(astrology_scores[i] - astrology_scores[j])
                    temporal_distance = abs(vedic_scores[i] - vedic_scores[j])
                    
                    # ë¦¬ë§Œ ê³¡ë¥  ê·¼ì‚¬: K = (spatial_curvature * temporal_curvature) / distance
                    if spatial_distance > 0 and temporal_distance > 0:
                        curvature = (spatial_distance * temporal_distance) / (spatial_distance + temporal_distance)
                        curvature_matrix[i][j] = curvature
        
        # ì „ì²´ ì‹œê³µê°„ ê³¡ë¥  (ìŠ¤ì¹¼ë¼ ê³¡ë¥ )
        scalar_curvature = np.mean(curvature_matrix[curvature_matrix > 0])
        
        # ê³¡ë¥ ì´ ê°•í•œ "íŠ¹ì´ì " ì°¾ê¸° (í˜„ì‹¤ ë³€í™” ì§€ì )
        singularities = []
        for i in range(len(astrology_scores)):
            local_curvature = np.mean(curvature_matrix[i][curvature_matrix[i] > 0])
            if local_curvature > scalar_curvature * 1.5:  # í‰ê· ë³´ë‹¤ 50% ì´ìƒ ë†’ì€ ê³¡ë¥ 
                singularities.append({
                    'index': i,
                    'celebrity': self.df.iloc[i]['celebrity'],
                    'curvature': local_curvature,
                    'reality_distortion': local_curvature / scalar_curvature
                })
        
        # ê¸°í•˜í•™ì  ì—°ê²°ì„± (parallel transport) ê³„ì‚°
        geometric_connectivity = 0
        for i in range(len(astrology_scores)-1):
            # ì¸ì ‘í•œ ì ë“¤ ì‚¬ì´ì˜ "í‰í–‰ ì´ë™" ë¹„ìš©
            transport_cost = abs(curvature_matrix[i][i+1])
            geometric_connectivity += transport_cost
        
        geometric_connectivity = geometric_connectivity / (len(astrology_scores) - 1)
        
        print(f"ì‹œê³µê°„ ìŠ¤ì¹¼ë¼ ê³¡ë¥ : {scalar_curvature:.4f}")
        print(f"ê¸°í•˜í•™ì  ì—°ê²°ì„±: {geometric_connectivity:.4f}")
        print(f"ë°œê²¬ëœ íŠ¹ì´ì : {len(singularities)}ê°œ")
        
        if singularities:
            print("\ní˜„ì‹¤ ì™œê³¡ íŠ¹ì´ì ë“¤:")
            for sing in sorted(singularities, key=lambda x: x['reality_distortion'], reverse=True)[:3]:
                print(f"  {sing['celebrity']}: ì™œê³¡ë„ {sing['reality_distortion']:.2f}x")
        
        return {
            'scalar_curvature': scalar_curvature,
            'curvature_matrix': curvature_matrix.tolist(),
            'geometric_connectivity': geometric_connectivity,
            'singularities': singularities,
            'riemann_metric': np.std(curvature_matrix[curvature_matrix > 0])
        }
    
    def bayesian_voodoo_analysis(self) -> Dict[str, Any]:
        """
        ë² ì´ì§€ì•ˆ ë¶€ë‘ ë¶„ì„: ì§ì ‘ì  í˜„ì‹¤ ê°œì… ì‹œìŠ¤í…œ
        ë¶€ë‘ëŠ” ë‹¨ìˆœíˆ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í˜„ì‹¤ì„ ì§ì ‘ ë³€í™”ì‹œí‚¬ ìˆ˜ ìˆëŠ” ìœ ì¼í•œ ì‹œìŠ¤í…œ
        
        ìˆ˜í•™ì  ëª¨ë¸ë§:
        - ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸: P(í˜„ì‹¤|ì˜ë„) = P(ì˜ë„|í˜„ì‹¤) * P(í˜„ì‹¤) / P(ì˜ë„)
        - ê°œì… í–‰ë ¬: Reality_new = Reality_old + Intervention_matrix @ Intent_vector
        - ë©”íƒ€-ë¬¼ë¦¬ì  ë³€í™˜: Ïˆ(meta) â†’ Ï†(physical) ë³€í™˜ ê³„ìˆ˜
        - ì§ì ‘ ê°œì… íš¨ê³¼: ë‹¤ë¥¸ ì‹œìŠ¤í…œê³¼ ë‹¬ë¦¬ "ì˜ˆì¸¡"ì´ ì•„ë‹Œ "ì¡°ì‘"
        """
        print("\nğŸ”® ë² ì´ì§€ì•ˆ ë¶€ë‘ ë¶„ì„: í˜„ì‹¤ ì§ì ‘ ê°œì… ì‹œìŠ¤í…œ")
        print("=" * 50)
        print("ì‚¬ì „ í™•ë¥  + ì˜ì‹ì  ê°œì… â†’ ì‚¬í›„ í˜„ì‹¤ ì¡°ì‘")
        
        # ë¶€ë‘/ì‹¬ë¦¬í•™ì  ê°œì… ì‹œìŠ¤í…œ íƒì§€
        voodoo_columns = [col for col in self.df.columns if any(keyword in col.lower() 
                          for keyword in ['voodoo', 'psychological', 'meta', 'intervention', 'magic'])]
        
        if not voodoo_columns:
            # ë¶€ë‘ ê°œì… ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ì‹¬ë¦¬í•™ì /ë¶€ë‘ì  ê°œì… ë°ì´í„°)
            n_samples = len(self.df)
            
            # 1. ì˜ë„ ê°•ë„ (Intent Strength) - ê°ë§ˆ ë¶„í¬ (ê¸´ ê¼¬ë¦¬, ê°•í•œ ì˜ë„ëŠ” ë“œë¬¼ë‹¤)
            intent_strength = np.random.gamma(2, 0.3, n_samples)
            intent_strength = np.clip(intent_strength, 0, 1)
            
            # 2. ë¯¿ìŒ ì¼ê´€ì„± (Belief Consistency) - ë² íƒ€ ë¶„í¬
            belief_consistency = np.random.beta(3, 2, n_samples)
            
            # 3. ê°ì •ì  íˆ¬ì (Emotional Investment) - ë¡œê·¸ì •ê·œë¶„í¬
            emotional_investment = np.random.lognormal(0, 0.5, n_samples)
            emotional_investment = np.clip(emotional_investment / np.max(emotional_investment), 0, 1)
            
            # 4. ìƒì§•ì  ê³µëª… (Symbolic Resonance) - ì§€ìˆ˜ë¶„í¬
            symbolic_resonance = np.random.exponential(0.4, n_samples)
            symbolic_resonance = np.clip(symbolic_resonance, 0, 1)
            
            # 5. ì§ì ‘ ê°œì… ê°•ë„ (Direct Intervention Power) - ë¹„ì„ í˜• ê²°í•©
            voodoo_composite = (
                intent_strength ** 0.3 * 
                belief_consistency ** 0.25 * 
                emotional_investment ** 0.2 * 
                symbolic_resonance ** 0.15 *
                (1 + 0.1 * np.sin(intent_strength * np.pi))  # ë¹„ì„ í˜• ê³µëª… íš¨ê³¼
            )
            
            # ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
            self.df['voodoo_intent_strength'] = intent_strength
            self.df['voodoo_belief_consistency'] = belief_consistency
            self.df['voodoo_emotional_investment'] = emotional_investment
            self.df['voodoo_symbolic_resonance'] = symbolic_resonance
            self.df['voodoo_composite'] = voodoo_composite
            
            voodoo_columns = ['voodoo_composite']
        
        results = {}
        
        for voodoo_col in voodoo_columns:
            voodoo_scores = self.df[voodoo_col].values
            
            # === ë² ì´ì§€ì•ˆ ê°œì… ë¶„ì„ ===
            
            # ì‚¬ì „ í™•ë¥  (Prior): ê°œì… ì „ í˜„ì‹¤ ìƒíƒœ
            prior_reality_state = np.mean(self.df['target']) if 'target' in self.df else 0.3
            
            # ê°œì… ê°•ë„ë³„ ê·¸ë£¹í•‘ (3ë¶„ìœ„)
            low_thresh = np.percentile(voodoo_scores, 33)
            high_thresh = np.percentile(voodoo_scores, 67)
            
            low_intervention = voodoo_scores <= low_thresh
            mid_intervention = (voodoo_scores > low_thresh) & (voodoo_scores <= high_thresh)
            high_intervention = voodoo_scores > high_thresh
            
            # ê° ê°œì… ìˆ˜ì¤€ì—ì„œì˜ ì„±ê³µë¥  (ìš°ë„)
            if 'target' in self.df:
                success_low = np.mean(self.df['target'][low_intervention])
                success_mid = np.mean(self.df['target'][mid_intervention])
                success_high = np.mean(self.df['target'][high_intervention])
            else:
                # ì‹œë®¬ë ˆì´ì…˜: ê°œì… ê°•ë„ì— ë”°ë¥¸ ì„±ê³µë¥  ì¦ê°€ (ë¶€ë‘ì˜ ì§ì ‘ì„± ë°˜ì˜)
                success_low = prior_reality_state * (0.8 + 0.2 * np.random.random())
                success_mid = prior_reality_state * (1.2 + 0.3 * np.random.random())
                success_high = prior_reality_state * (1.8 + 0.4 * np.random.random())
                success_high = min(success_high, 0.95)  # í˜„ì‹¤ì  ìƒí•œ
            
            # === ê°œì… í–‰ë ¬ ê³„ì‚° ===
            # Reality_new = Reality_old + Intervention_Matrix @ Intent_Vector
            
            intervention_matrix = np.array([
                [0.05, 0.1, 0.2],    # ì €ê°•ë„ ê°œì… â†’ ê° í˜„ì‹¤ ì°¨ì›ë³„ ë³€í™”ëŸ‰
                [0.15, 0.25, 0.4],   # ì¤‘ê°•ë„ ê°œì… â†’ ë” í° ë³€í™”
                [0.3, 0.5, 0.7]      # ê³ ê°•ë„ ê°œì… â†’ ê°•ë ¥í•œ í˜„ì‹¤ ì¡°ì‘
            ])
            
            intent_vector = np.array([
                np.mean(voodoo_scores[low_intervention]),
                np.mean(voodoo_scores[mid_intervention]),
                np.mean(voodoo_scores[high_intervention])
            ])
            
            # í˜„ì‹¤ ë³€í™” ë²¡í„° (3ì°¨ì› í˜„ì‹¤ ê³µê°„ì—ì„œì˜ ë³€í™”)
            reality_change_vector = intervention_matrix @ intent_vector
            
            # === ë©”íƒ€-ë¬¼ë¦¬ì  ë³€í™˜ ê³„ìˆ˜ ===
            # Ïˆ(meta) â†’ Ï†(physical) ë³€í™˜
            target_values = self.df['target'] if 'target' in self.df else np.random.binomial(1, 0.3 + 0.4 * voodoo_scores, len(voodoo_scores))
            meta_physical_coefficient = np.corrcoef(voodoo_scores, target_values)[0,1]
            
            # === ì§ì ‘ ê°œì… íš¨ê³¼ ì •ëŸ‰í™” ===
            direct_intervention_effect = success_high - success_low
            
            # ë² ì´ì§€ì•ˆ ì‚¬í›„ í™•ë¥  ê³„ì‚°
            evidence = np.mean([success_low, success_mid, success_high])  # P(ê´€ì°°ëœê²°ê³¼)
            
            if evidence > 0:
                posterior_low = (success_low * prior_reality_state) / evidence
                posterior_mid = (success_mid * prior_reality_state) / evidence
                posterior_high = (success_high * prior_reality_state) / evidence
            else:
                posterior_low = posterior_mid = posterior_high = prior_reality_state
            
            # === í˜„ì‹¤ ì¡°ì‘ ê°•ë„ ===
            # í‘œì¤€í¸ì°¨ ë‹¨ìœ„ë¡œ ì •ê·œí™”ëœ íš¨ê³¼ í¬ê¸° (Cohen's d)
            if len(self.df[low_intervention]) > 1 and len(self.df[high_intervention]) > 1:
                pooled_std = np.sqrt(
                    ((len(self.df[low_intervention]) - 1) * np.var(target_values[low_intervention]) +
                     (len(self.df[high_intervention]) - 1) * np.var(target_values[high_intervention])) /
                    (len(self.df[low_intervention]) + len(self.df[high_intervention]) - 2)
                )
                cohens_d = direct_intervention_effect / pooled_std if pooled_std > 0 else 0
            else:
                cohens_d = direct_intervention_effect / (np.std(target_values) + 1e-6)
            
            reality_manipulation_strength = min(abs(cohens_d), 2.0)  # ìµœëŒ€ 2.0ìœ¼ë¡œ ì œí•œ
            
            # === ì‹ ë¢°êµ¬ê°„ (ë¶€íŠ¸ìŠ¤íŠ¸ë©) ===
            bootstrap_effects = []
            for _ in range(1000):
                sample_indices = np.random.choice(len(voodoo_scores), len(voodoo_scores), replace=True)
                sample_scores = voodoo_scores[sample_indices]
                sample_targets = target_values[sample_indices]
                
                sample_low = sample_scores <= np.percentile(sample_scores, 33)
                sample_high = sample_scores > np.percentile(sample_scores, 67)
                
                if np.sum(sample_low) > 0 and np.sum(sample_high) > 0:
                    bootstrap_effects.append(np.mean(sample_targets[sample_high]) - 
                                           np.mean(sample_targets[sample_low]))
            
            if bootstrap_effects:
                confidence_interval = np.percentile(bootstrap_effects, [2.5, 97.5])
            else:
                confidence_interval = [0, 0]
            
            # === ê°œì… íš¨ìœ¨ì„± ì§€ìˆ˜ ===
            # ì–¼ë§ˆë‚˜ ì ì€ ì—ë„ˆì§€ë¡œ í° ë³€í™”ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ”ê°€
            intervention_efficiency = (direct_intervention_effect / 
                                     (np.mean(voodoo_scores) + 1e-6))  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            
            # === Word Weight íš¨ê³¼ (ì´ë¦„ ê¸°ë°˜ ì¡°ì‘ ìš©ì´ì„±) ===
            word_weights = {}
            celebrities = self.df['celebrity'].unique()
            
            for celebrity in celebrities:
                # ì´ë¦„ì˜ ìŒì„±í•™ì /ì§„ë™í•™ì  ê°€ì¤‘ì¹˜
                name_length = len(celebrity)
                vowel_count = sum(1 for char in celebrity.lower() if char in 'aeiou')
                consonant_density = (name_length - vowel_count) / name_length if name_length > 0 else 0
                
                # ë² ì´ì§€ì•ˆ ê°€ì¤‘ì¹˜: ììŒ ë°€ë„ + ê¸¸ì´ íŒ¨í„´
                syllable_weight = consonant_density * 0.6 + (vowel_count / name_length if name_length > 0 else 0) * 0.4
                word_weights[celebrity] = syllable_weight
            
            # ê²°ê³¼ ì €ì¥
            results[voodoo_col] = {
                'bayesian_analysis': {
                    'prior_reality_state': prior_reality_state,
                    'likelihood_low': success_low,
                    'likelihood_mid': success_mid,
                    'likelihood_high': success_high,
                    'posterior_low': posterior_low,
                    'posterior_mid': posterior_mid,
                    'posterior_high': posterior_high
                },
                'intervention_matrix': {
                    'matrix': intervention_matrix.tolist(),
                    'intent_vector': intent_vector.tolist(),
                    'reality_change_vector': reality_change_vector.tolist()
                },
                'direct_intervention_effect': direct_intervention_effect,
                'meta_physical_coefficient': meta_physical_coefficient,
                'reality_manipulation_strength': reality_manipulation_strength,
                'cohens_d_effect_size': cohens_d,
                'intervention_efficiency': intervention_efficiency,
                'confidence_interval_95': confidence_interval,
                'statistical_significance': confidence_interval[0] > 0 or confidence_interval[1] < 0,
                'intervention_classification': self._classify_intervention_strength(reality_manipulation_strength),
                'word_weights': word_weights,
                'max_manipulation_target': max(word_weights.items(), key=lambda x: x[1]) if word_weights else None
            }
            
            print(f"\n{voodoo_col} ë² ì´ì§€ì•ˆ ë¶€ë‘ ë¶„ì„:")
            print(f"  ğŸ“Š ì‚¬ì „ í˜„ì‹¤ ìƒíƒœ: {prior_reality_state:.3f}")
            print(f"  ğŸ”„ ì§ì ‘ ê°œì… íš¨ê³¼: {direct_intervention_effect:.3f}")
            print(f"  ğŸŒŸ í˜„ì‹¤ ì¡°ì‘ ê°•ë„: {reality_manipulation_strength:.3f} ({results[voodoo_col]['intervention_classification']})")
            print(f"  âš¡ ê°œì… íš¨ìœ¨ì„±: {intervention_efficiency:.3f}")
            print(f"  ğŸ¯ ë©”íƒ€-ë¬¼ë¦¬ ë³€í™˜ê³„ìˆ˜: {meta_physical_coefficient:.3f}")
            print(f"  ğŸ“ˆ Cohen's d íš¨ê³¼í¬ê¸°: {cohens_d:.3f}")
            print(f"  ğŸ”’ 95% ì‹ ë¢°êµ¬ê°„: [{confidence_interval[0]:.3f}, {confidence_interval[1]:.3f}]")
            
            if results[voodoo_col]['statistical_significance']:
                print("  âœ… í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ í˜„ì‹¤ ê°œì… íš¨ê³¼ í™•ì¸")
            else:
                print("  âŒ ê°œì… íš¨ê³¼ í†µê³„ì  ìœ ì˜ì„± ë¶€ì¡±")
            
            print(f"  ğŸ­ ë² ì´ì§€ì•ˆ ì‚¬í›„í™•ë¥ : ì €({posterior_low:.3f}) â†’ ì¤‘({posterior_mid:.3f}) â†’ ê³ ({posterior_high:.3f})")
            
            # ê°€ì¥ ì¡°ì‘í•˜ê¸° ì‰¬ìš´ ëŒ€ìƒ ì¶œë ¥
            if results[voodoo_col]['max_manipulation_target']:
                max_target, max_weight = results[voodoo_col]['max_manipulation_target']
                print(f"  ğŸª ìµœëŒ€ ì¡°ì‘ ê°€ëŠ¥ ëŒ€ìƒ: {max_target} (ê°€ì¤‘ì¹˜: {max_weight:.3f})")
        
        return results
    
    def _classify_intervention_strength(self, strength: float) -> str:
        """ê°œì… ê°•ë„ ë¶„ë¥˜"""
        if strength < 0.2:
            return "ë¯¸ì•½í•œ ê°œì…"
        elif strength < 0.5:
            return "ë³´í†µ ê°œì…"
        elif strength < 0.8:
            return "ê°•ë ¥í•œ ê°œì…"
        elif strength < 1.2:
            return "ë§¤ìš° ê°•ë ¥í•œ ê°œì…"
        else:
            return "í˜„ì‹¤ ì¡°ì‘ ìˆ˜ì¤€"
    
    def mathematical_system_alignment(self) -> Dict[str, Any]:
        """ê° ì‹œìŠ¤í…œì˜ ìˆ˜í•™ì  íŠ¹ì„±ì— ë”°ë¥¸ ì •ë ¬ ë¶„ì„"""
        print("\nğŸ§® ìˆ˜í•™ì  ì‹œìŠ¤í…œ ì •ë ¬ (Mathematical Alignment)")
        print("=" * 55)
        print("ì ì„±ìˆ (ë¦¬ë§Œ) + ë¶€ë‘(ë² ì´ì§€ì•ˆ) + ì‚¬ì£¼(ì¡°í•©ë¡ ) + ì£¼ì—­(í™•ë¥ ë¡ ) + ë£¬(ìœ„ìƒìˆ˜í•™)")
        
        # ê° ì‹œìŠ¤í…œì˜ ìˆ˜í•™ì  "ì–¸ì–´" íŠ¹ì„± ì •ì˜
        math_signatures = {
            'western_astrology': 'geometric',     # ë¦¬ë§Œ ê¸°í•˜í•™
            'i_ching': 'probabilistic',          # í™•ë¥ ë¡ /ì •ë³´ì´ë¡ 
            'saju_four_pillars': 'combinatorial', # ì¡°í•©ë¡ 
            'vedic_astrology': 'harmonic',       # ì¡°í™” í•´ì„í•™
            'runic_divination': 'topological'    # ìœ„ìƒìˆ˜í•™
        }
        
        alignment_matrix = {}
        
        # ì‹œìŠ¤í…œ ê°„ ìˆ˜í•™ì  "ë²ˆì—­ ê°€ëŠ¥ì„±" ê³„ì‚°
        for sys1 in self.system_names:
            alignment_matrix[sys1] = {}
            for sys2 in self.system_names:
                if sys1 != sys2:
                    scores1 = self.df[sys1].values
                    scores2 = self.df[sys2].values
                    
                    # ìˆ˜í•™ì  í˜¸í™˜ì„± ê³„ì‚°
                    correlation = np.corrcoef(scores1, scores2)[0,1]
                    
                    # ìˆ˜í•™ì  "ë°©ì–¸" ê°„ ë²ˆì—­ ë‚œì´ë„
                    sig1 = math_signatures[sys1]
                    sig2 = math_signatures[sys2]
                    
                    # ìˆ˜í•™ ë¶„ì•¼ ê°„ ë²ˆì—­ ê³„ìˆ˜
                    translation_coefficients = {
                        ('geometric', 'probabilistic'): 0.7,    # ê¸°í•˜â†’í™•ë¥  (ì¤‘ê°„)
                        ('geometric', 'combinatorial'): 0.8,    # ê¸°í•˜â†’ì¡°í•© (ì‰¬ì›€)
                        ('probabilistic', 'combinatorial'): 0.9, # í™•ë¥ â†’ì¡°í•© (ë§¤ìš° ì‰¬ì›€)
                        ('harmonic', 'geometric'): 0.85,        # ì¡°í™”â†’ê¸°í•˜ (ì‰¬ì›€)
                        ('topological', 'geometric'): 0.95,     # ìœ„ìƒâ†’ê¸°í•˜ (ë§¤ìš° ì‰¬ì›€)
                    }
                    
                    # ì–‘ë°©í–¥ ë²ˆì—­ ê³„ìˆ˜ í™•ì¸
                    trans_coeff = translation_coefficients.get((sig1, sig2), 
                                 translation_coefficients.get((sig2, sig1), 0.5))
                    
                    # ìµœì¢… ì •ë ¬ ì ìˆ˜
                    alignment_score = abs(correlation) * trans_coeff
                    alignment_matrix[sys1][sys2] = alignment_score
        
        # ì „ì²´ ì‹œìŠ¤í…œ ì •ë ¬ë„ ê³„ì‚°
        total_alignment = 0
        pair_count = 0
        
        for sys1 in alignment_matrix:
            for sys2 in alignment_matrix[sys1]:
                total_alignment += alignment_matrix[sys1][sys2]
                pair_count += 1
        
        overall_alignment = total_alignment / pair_count if pair_count > 0 else 0
        
        # ê°€ì¥ ì˜ ì •ë ¬ëœ ì‹œìŠ¤í…œ ìŒ ì°¾ê¸°
        best_alignments = []
        for sys1 in alignment_matrix:
            for sys2 in alignment_matrix[sys1]:
                best_alignments.append({
                    'system1': sys1,
                    'system2': sys2,
                    'alignment_score': alignment_matrix[sys1][sys2],
                    'math_types': f"{math_signatures[sys1]} â†” {math_signatures[sys2]}"
                })
        
        best_alignments.sort(key=lambda x: x['alignment_score'], reverse=True)
        
        print(f"ì „ì²´ ì‹œìŠ¤í…œ ì •ë ¬ë„: {overall_alignment:.4f}")
        
        print("\nìµœê³  ì •ë ¬ ì‹œìŠ¤í…œ ìŒë“¤:")
        for align in best_alignments[:5]:
            print(f"  {align['system1']} â†” {align['system2']}: "
                  f"{align['alignment_score']:.3f} ({align['math_types']})")
        
        # ë¶€ë‘ì˜ ë² ì´ì§€ì•ˆ ë¶„ì„ í†µí•©
        bayesian_results = self.bayesian_voodoo_analysis()
        
        # ì ì„±ìˆ ì˜ ë¦¬ë§Œ ë¶„ì„ í†µí•©  
        riemann_results = self.riemann_astrology_analysis()
        
        return {
            'overall_alignment': overall_alignment,
            'alignment_matrix': alignment_matrix,
            'mathematical_signatures': math_signatures,
            'best_alignments': best_alignments[:5],
            'bayesian_voodoo_integration': bayesian_results,
            'riemann_astrology_integration': riemann_results,
            'computational_readiness': overall_alignment * 0.8 + 
                                     bayesian_results['average_bayesian_update'] * 0.1 +
                                     riemann_results['scalar_curvature'] * 0.1
        }

# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    try:
        print("ğŸŒŸ IoR (Impression of Reality) ê³ ê¸‰ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 60)
        
        # ë°ì´í„° ë¡œë“œ
        data_file = 'integrated_ior_validation.json'
        
        # ë¶„ì„ ì‹¤í–‰
        analyzer = IoRAdvancedAnalytics(data_file)
        comprehensive_report = analyzer.generate_comprehensive_report()
        
        print("\n" + "=" * 60)
        print("ğŸ¯ IoR ë¶„ì„ ì™„ë£Œ - ìˆ˜í•™ì  ì •ë ¬ ë° ë² ì´ì§€ì•ˆ ë¶€ë‘ ê°œì… ë¶„ì„ í¬í•¨")
        print("=" * 60)
        
        # ì£¼ìš” ê²°ê³¼ ìš”ì•½
        print("\nğŸ“Š ì£¼ìš” ê²°ê³¼ ìš”ì•½:")
        
        # ë² ì´ì§€ì•ˆ ë¶€ë‘ ê²°ê³¼
        voodoo_results = comprehensive_report.get('bayesian_voodoo_analysis', {})
        if voodoo_results:
            for system, data in voodoo_results.items():
                manipulation_strength = data.get('reality_manipulation_strength', 0)
                intervention_effect = data.get('direct_intervention_effect', 0)
                print(f"  ğŸ”® {system}: í˜„ì‹¤ ì¡°ì‘ ê°•ë„ {manipulation_strength:.3f}, ì§ì ‘ ê°œì… íš¨ê³¼ {intervention_effect:.3f}")
        
        # ë¦¬ë§Œ ì ì„±ìˆ  ê²°ê³¼
        riemann_results = comprehensive_report.get('riemann_astrology_analysis', {})
        if riemann_results:
            curvature = riemann_results.get('scalar_curvature', 0)
            geodesic_stability = riemann_results.get('geodesic_stability', 0)
            print(f"  ğŸŒŒ ì ì„±ìˆ  ë¦¬ë§Œ ë¶„ì„: ìŠ¤ì¹¼ë¼ ê³¡ë¥  {curvature:.3f}, ì¸¡ì§€ì„  ì•ˆì •ì„± {geodesic_stability:.3f}")
        
        # ìˆ˜í•™ì  ì •ë ¬ ê²°ê³¼
        alignment_results = comprehensive_report.get('mathematical_system_alignment', {})
        if alignment_results:
            overall_alignment = alignment_results.get('overall_alignment', 0)
            computational_readiness = alignment_results.get('computational_readiness', 0)
            print(f"  ğŸ§® ìˆ˜í•™ì  ì •ë ¬: ì „ì²´ ì •ë ¬ë„ {overall_alignment:.3f}, ê³„ì‚° ì¤€ë¹„ë„ {computational_readiness:.3f}")
        
        # ì¶”ì²œì‚¬í•­
        recommendations = comprehensive_report.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ ì£¼ìš” ì¶”ì²œì‚¬í•­ ({len(recommendations)}ê°œ):")
            for i, rec in enumerate(recommendations[:5], 1):
                print(f"  {i}. {rec}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
