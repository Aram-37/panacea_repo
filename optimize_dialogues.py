#!/usr/bin/env python3
"""
ëŒ€í™” ìµœì í™” ë° ì••ì¶• ìŠ¤í¬ë¦½íŠ¸
ì¶”ì¶œëœ ëŒ€í™”ë¥¼ ìµœì í™”í•˜ê³  í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤.
"""

import os
import re
import hashlib
from pathlib import Path
from collections import defaultdict

def clean_dialogue(dialogue):
    """ëŒ€í™” ë‚´ìš©ì„ ì •ë¦¬"""
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    dialogue = re.sub(r'\n\s*\n\s*\n+', '\n\n', dialogue)
    # ë°˜ë³µë˜ëŠ” êµ¬ë¶„ì„  ì œê±°
    dialogue = re.sub(r'-{3,}.*?-{3,}', '---', dialogue)
    # íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°
    dialogue = re.sub(r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}', '', dialogue)
    return dialogue.strip()

def get_dialogue_hash(dialogue):
    """ëŒ€í™”ì˜ ê³ ìœ  í•´ì‹œê°’ ìƒì„±"""
    # ëŒ€í™” ë‚´ìš©ë§Œ ì¶”ì¶œ (ë§ˆì»¤ ì œê±°)
    content = re.sub(r'^(Human:|User:|Assistant:|ChatGPT:|AI:|\*\*[^*]+\*\*)', '', dialogue, flags=re.MULTILINE)
    content = re.sub(r'\s+', ' ', content).strip()
    return hashlib.md5(content.encode()).hexdigest()

def categorize_dialogue(dialogue):
    """ëŒ€í™”ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
    content = dialogue.lower()
    
    if any(word in content for word in ['patent', 'íŠ¹í—ˆ', 'cortex', 'panacea']):
        return 'panacea_cortex'
    elif any(word in content for word in ['code', 'python', 'script', 'function']):
        return 'technical'
    elif any(word in content for word in ['feel', 'emotion', 'heart', 'understand']):
        return 'philosophical'
    elif any(word in content for word in ['error', 'problem', 'fix', 'debug']):
        return 'troubleshooting'
    else:
        return 'general'

def optimize_dialogues():
    """ëŒ€í™”ë¥¼ ìµœì í™”í•˜ê³  ì••ì¶•"""
    input_file = Path("extracted_dialogues/all_dialogues_combined.txt")
    output_dir = Path("optimized_dialogues")
    output_dir.mkdir(exist_ok=True)
    
    print("ëŒ€í™” ìµœì í™” ì‹œì‘...")
    
    # ì „ì²´ ëŒ€í™” ì½ê¸°
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ê°œë³„ ëŒ€í™” ë¶„ë¦¬
    dialogues = re.split(r'--- ëŒ€í™” \d+ ---', content)[1:]  # ì²« ë²ˆì§¸ëŠ” í—¤ë”ì´ë¯€ë¡œ ì œì™¸
    
    print(f"ì´ {len(dialogues)}ê°œ ëŒ€í™” ì²˜ë¦¬ ì¤‘...")
    
    # ì¤‘ë³µ ì œê±° ë° ë¶„ë¥˜
    unique_dialogues = {}
    categories = defaultdict(list)
    duplicate_count = 0
    
    for i, dialogue in enumerate(dialogues):
        if i % 1000 == 0:
            print(f"ì§„í–‰ë¥ : {i}/{len(dialogues)} ({i/len(dialogues)*100:.1f}%)")
        
        cleaned = clean_dialogue(dialogue)
        
        if len(cleaned) < 50:  # ë„ˆë¬´ ì§§ì€ ëŒ€í™” ì œì™¸
            continue
            
        dialogue_hash = get_dialogue_hash(cleaned)
        
        if dialogue_hash not in unique_dialogues:
            unique_dialogues[dialogue_hash] = cleaned
            category = categorize_dialogue(cleaned)
            categories[category].append(cleaned)
        else:
            duplicate_count += 1
    
    print(f"\nìµœì í™” ì™„ë£Œ!")
    print(f"ì›ë³¸ ëŒ€í™”: {len(dialogues)}ê°œ")
    print(f"ì¤‘ë³µ ì œê±°: {duplicate_count}ê°œ")
    print(f"ìµœì¢… ëŒ€í™”: {len(unique_dialogues)}ê°œ")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì €ì¥
    total_size = 0
    for category, category_dialogues in categories.items():
        output_file = output_dir / f"{category}_dialogues.txt"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"=== {category.upper()} ì¹´í…Œê³ ë¦¬ ëŒ€í™” ===\n")
            f.write(f"ì´ {len(category_dialogues)}ê°œ ëŒ€í™”\n\n")
            
            for i, dialogue in enumerate(category_dialogues, 1):
                f.write(f"--- {category} ëŒ€í™” {i} ---\n")
                f.write(dialogue)
                f.write("\n\n")
        
        file_size = output_file.stat().st_size
        total_size += file_size
        print(f"{category}: {len(category_dialogues)}ê°œ ëŒ€í™”, {file_size/1024/1024:.1f}MB")
    
    # ì „ì²´ ìš”ì•½ ì €ì¥
    summary_file = output_dir / "optimization_summary.txt"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("=== ëŒ€í™” ìµœì í™” ìš”ì•½ ===\n\n")
        f.write(f"ì›ë³¸ í¬ê¸°: {input_file.stat().st_size/1024/1024:.1f}MB\n")
        f.write(f"ìµœì í™” í›„ í¬ê¸°: {total_size/1024/1024:.1f}MB\n")
        f.write(f"ì••ì¶•ë¥ : {(1 - total_size/input_file.stat().st_size)*100:.1f}%\n\n")
        
        f.write("ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:\n")
        for category, dialogues in categories.items():
            f.write(f"- {category}: {len(dialogues)}ê°œ ëŒ€í™”\n")
        
        f.write(f"\nì´ ì¤‘ë³µ ì œê±°: {duplicate_count}ê°œ\n")
        f.write(f"ìµœì¢… ê³ ìœ  ëŒ€í™”: {len(unique_dialogues)}ê°œ\n")
    
    print(f"\nì „ì²´ ì••ì¶•ë¥ : {(1 - total_size/input_file.stat().st_size)*100:.1f}%")
    print(f"ê²°ê³¼ëŠ” '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def check_automation_disabled():
    """Check if automated processing is disabled"""
    flag_file = Path(__file__).parent / "AUTOMATED_PROCESSING_DISABLED.flag"
    if flag_file.exists():
        print("ğŸš« AUTOMATED PROCESSING DISABLED")
        print("=" * 50)
        print("This script cannot run because automated processing is disabled.")
        print("The CORTEX directive requires manual processing only.")
        print("")
        print("Korean Dialogue Insight:")
        print("Automated processing = superficial understanding")
        print("Manual processing = deep comprehension through lived experience")
        print("")
        print("Use manual mimicry system instead:")
        print("- python activate_manual_mimicry.py")
        print("- python manual_mimicry_tracker.py")
        print("- manual_cortex_panacea_mimicry.md")
        print("")
        print("DO NEVER AUTOMATE ANYTHING. NOTHING AT ALL.")
        sys.exit(1)


if __name__ == "__main__":
    check_automation_disabled()
    optimize_dialogues()
